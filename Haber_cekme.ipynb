{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpozHd9nbcUKDZlEF+pxy9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/-Calculating-potential-customer-revenue-using-rule-based-classification/blob/main/Haber_cekme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install pandas\n",
        "!pip install datetime\n",
        "!pip install win32com"
      ],
      "metadata": {
        "id": "g1Di_a8rTLnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from openai import OpenAI  # OpenAI API için gerekli\n",
        "import base64\n",
        "import requests\n",
        "\n",
        "# OpenAI API anahtarını ayarla\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key='sk-proj-',\n",
        ")\n",
        "\n",
        "# SerpAPI anahtarını ayarla\n",
        "SERP_API_KEY = \"\"\n",
        "\n",
        "# This function encodes the image to base64\n",
        "def get_image_base64(image_url):\n",
        "    try:\n",
        "        response = requests.get(image_url)\n",
        "        if response.status_code == 200:\n",
        "            return base64.b64encode(response.content).decode('utf-8')\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching image from {image_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Google'da haber aramak için SerpAPI kullan\n",
        "def search_aselsan_news(start_date, end_date):\n",
        "    params = {\n",
        "        \"q\": \"Aselsan haber OR Aselsan news\",\n",
        "        \"tbm\": \"nws\",\n",
        "        \"tbs\": f\"cdr:1,cd_min:{start_date},cd_max:{end_date}\",\n",
        "        \"api_key\": SERP_API_KEY\n",
        "    }\n",
        "    url = \"https://serpapi.com/search\"\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()\n",
        "\n",
        "    print(\"SerpAPI Yanıtı:\", response.json())\n",
        "\n",
        "    news_links = []\n",
        "    if 'news_results' in results:\n",
        "        for news in results['news_results']:\n",
        "            news_links.append({\n",
        "                'title': news['title'],\n",
        "                'link': news['link'],\n",
        "                'date': news['date'],\n",
        "                'source': news['source'],  # Haber kaynağı eklendi\n",
        "                'thumbnail': news['thumbnail']\n",
        "            })\n",
        "    return news_links\n",
        "\n",
        "# Web sayfasından haber içeriğini çek\n",
        "def scrape_news_content(news_url):\n",
        "    try:\n",
        "        response = requests.get(news_url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            paragraphs = soup.find_all('p')\n",
        "            content = ' '.join([p.get_text() for p in paragraphs])\n",
        "            return content\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {news_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# OpenAI API ile haber içeriğini özetle (gpt-4 modelini kullanarak)\n",
        "def summarize_content(content):\n",
        "    try:\n",
        "        # OpenAI API'yi kullanarak özetleme yapıyoruz\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[{\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Sen yardımsever çeviri ve özet uzmanısın.\"\n",
        "            }, {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Aşağıdaki yazıyı Türkçe ve maksimum 250 token olacak şekilde özetler misin? Bu özeti yöneticilerime ileteceğim.\\n\\n{content}\"\n",
        "            }],\n",
        "            max_tokens=750,\n",
        "            temperature=0.5\n",
        "        )\n",
        "        # Yanıtı kontrol edelim\n",
        "        if response and response.choices:\n",
        "            summary = response.choices[0].message.content.strip()  # Hatanın düzeltildiği yer\n",
        "            return summary\n",
        "        else:\n",
        "            return \"Özet oluşturulamadı.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error during summarization: {e}\")\n",
        "        return \"Özetleme sırasında bir hata oluştu.\"\n",
        "\n",
        "# Raporu CSV formatında kaydet (sütunları \";\" ile ayır)\n",
        "def save_news_to_csv(news_data):\n",
        "    df = pd.DataFrame(news_data)\n",
        "    file_name = f\"aselsan_news_report_{datetime.now().strftime('%Y-%m-%d')}.csv\"\n",
        "    df.to_csv(file_name, index=False, encoding='utf-8', sep=';')\n",
        "    print(f\"Rapor oluşturuldu: {file_name}\")\n",
        "\n",
        "# E-posta içeriği oluşturma (.eml formatında kaydetme)\n",
        "def create_eml_file(news_data, start_date, end_date):\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = 'your_email@example.com'\n",
        "    msg['To'] = 'recipient@example.com'\n",
        "    msg['Subject'] = f\"ASELSAN Bülteni ({start_date} - {end_date})\"\n",
        "\n",
        "    body = f\"<h1>ASELSAN Bülteni</h1>\"\n",
        "    body += f\"<p><strong>Tarih Aralığı:</strong> {start_date} - {end_date}</p>\"\n",
        "\n",
        "    for news in news_data:\n",
        "        body += f\"<h2>{news['Title']}</h2>\"\n",
        "        body += f\"<p><strong>Kaynak:</strong> {news['Source']}</p>\"\n",
        "\n",
        "        # Embed the image using base64\n",
        "        img_base64 = get_image_base64(news['Thumbnail'])\n",
        "        if img_base64:\n",
        "            body += f\"<img src='data:image/jpeg;base64,{img_base64}' alt='Haber Resmi' style='max-width:100%; width:1000px; height:auto;'><br>\"\n",
        "\n",
        "        body += f\"<p>{news['Summary']}</p>\"\n",
        "        body += f\"<a href='{news['Link']}'>Haberi Oku</a><br>\"\n",
        "        body += f\"<p><strong>Tarih:</strong> {news['Date']}</p><hr>\"\n",
        "\n",
        "    msg.attach(MIMEText(body, 'html'))\n",
        "\n",
        "    # .eml dosyası olarak kaydetme\n",
        "    eml_file_name = f\"aselsan_bulteni_{datetime.now().strftime('%Y-%m-%d')}.eml\"\n",
        "    with open(eml_file_name, 'w') as f:\n",
        "        f.write(msg.as_string())\n",
        "\n",
        "    print(f\"E-posta oluşturuldu ve kaydedildi: {eml_file_name}\")\n",
        "\n",
        "# Ana fonksiyon\n",
        "def get_aselsan_news_report(start_date, end_date):\n",
        "    news_links = search_aselsan_news(start_date, end_date)\n",
        "\n",
        "    if news_links:\n",
        "        news_data = []\n",
        "        for news in news_links:\n",
        "            print(f\"Processing: {news['title']}\")\n",
        "            content = scrape_news_content(news['link'])\n",
        "            if content:\n",
        "                summary = summarize_content(content)  # OpenAI API ile özet\n",
        "                news_data.append({\n",
        "                    'Title': news['title'],\n",
        "                    'Link': news['link'],\n",
        "                    'Date': news['date'],\n",
        "                    'Source': news['source'],  # Kaynak bilgisi eklendi\n",
        "                    'Summary': summary,\n",
        "                    'Thumbnail': news['thumbnail']  # Haber resmi eklendi\n",
        "                })\n",
        "        save_news_to_csv(news_data)\n",
        "        create_eml_file(news_data, start_date, end_date)  # EML dosyası oluşturma\n",
        "    else:\n",
        "        print(\"No news found for the given date range.\")\n",
        "\n",
        "# Tarih aralığını belirtin\n",
        "start_date = \"09/1/2024\"\n",
        "end_date = \"09/1/2024\"\n",
        "\n",
        "# Ana fonksiyonu çalıştır\n",
        "get_aselsan_news_report(start_date, end_date)\n"
      ],
      "metadata": {
        "id": "8-YXAmJMeJ3N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}